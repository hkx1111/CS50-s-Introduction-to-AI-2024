# 抽样与可能性加权

## 抽样基础

在贝叶斯网络中，抽样是一种近似推断技术，通过生成符合概率分布的随机样本来估计概率值。基本思想是多次随机生成样本，然后统计符合特定条件的样本比例，以此估计概率。

## 拒绝抽样的低效问题

文档中提到的抽样例子使用了一种称为"拒绝抽样"的方法：

```python
# 计算在火车延误的情况下 Appointment 的分布
N = 10000
data = []

# 重复抽样 10,000 次
for i in range(N):
    sample = generate_sample()
    # 丢弃不符合证据的样本
    if sample["train"] == "delayed":
        data.append(sample["appointment"])
```

这种方法的低效之处在于：
1. 我们生成了10,000个完整样本
2. 但只保留了符合"train = delayed"条件的样本
3. 其余不符合条件的样本都被丢弃了

如果火车延误概率很低（例如只有20%），那么大约80%的样本会被丢弃，造成计算资源的浪费。当我们有多个条件需要同时满足时，这种低效会更加明显。

## 可能性加权的解决方案

为解决这个问题，可能性加权方法提供了更高效的替代方案：

1. 固定证据变量值（例如，始终令Train="delayed"）
2. 只对非证据变量进行抽样
3. 对每个样本赋予权重，权重等于证据在其父节点条件下的条件概率

这种方法不会丢弃任何样本，而是通过权重来反映样本的可能性，从而更有效地利用计算资源。

# 马尔可夫链的生成

## 马尔可夫链的产生过程

马尔可夫链是基于转移模型生成的随机状态序列。以文中的天气预测例子为例：

1. 从初始状态开始（晴天或雨天，各有0.5的概率）
2. 根据当前状态和转移概率，确定下一个状态：
   - 如果今天是晴天，明天有0.8的概率是晴天，0.2的概率是雨天
   - 如果今天是雨天，明天有0.3的概率是晴天，0.7的概率是雨天
3. 重复此过程，生成一个状态序列

代码表示为：
```python
model = MarkovChain([start, transitions])
states = model.sample(50)  # 生成50个状态的序列
```

## 多次生成的结果差异

是的，程序多次创建马尔可夫链会产生不同的结果。这是因为：

1. 马尔可夫链的生成过程是**随机**的，每一步都根据概率分布进行抽样
2. 即使使用相同的初始状态和转移模型，随机抽样也会导致不同的状态序列
3. 只有在极长的序列中，各状态出现的频率才会接近其理论概率分布

例如，连续运行`model.sample(50)`多次，会得到不同的50天天气序列。这正是马尔可夫模型的特点：它们遵循概率规律，但每次实现都是随机的、独特的路径。

这种随机性使马尔可夫模型能够模拟真实世界中的随机过程，比如天气变化、股票价格波动或用户行为等。

---

# 可能性加权详解：从零开始理解

## 基本问题情境

让我用一个简化的例子来解释可能性加权的优势。假设我们有以下贝叶斯网络模型：

```
Rain → Train → Appointment
```

其中:
- Rain (下雨): {yes, no}
- Train (火车): {on_time, delayed}
- Appointment (约会): {attend, miss}

## 拒绝抽样过程

假设我们想知道：**已知火车延误，我们参加或错过约会的概率分布是什么？**

使用拒绝抽样，流程是这样的：

```python
# 假设我们要生成1000个样本
samples = []
accepted_count = 0

for i in range(1000):
    # 1. 先抽样Rain (根据其概率分布)
    rain = sample_rain()  # 可能是"yes"或"no"
    
    # 2. 再抽样Train (基于rain的条件)
    train = sample_train(rain)  # 可能是"on_time"或"delayed"
    
    # 3. 再抽样Appointment (基于train的条件)
    appointment = sample_appointment(train)
    
    # 4. 如果不符合证据(train="delayed")，丢弃整个样本
    if train == "delayed":
        samples.append(appointment)
        accepted_count += 1
```

**效率问题：** 如果火车只有20%的概率延误，那么大约80%的样本会被丢弃！

## 可能性加权过程

使用可能性加权，流程变为：

```python
weighted_samples = {"attend": 0, "miss": 0}
total_weight = 0

for i in range(1000):
    # 1. 先抽样Rain (根据其概率分布)
    rain = sample_rain()  # 可能是"yes"或"no"
    
    # 2. 固定Train="delayed"(不抽样)
    train = "delayed"
    
    # 3. 抽样Appointment (基于train="delayed"的条件)
    appointment = sample_appointment(train)
    
    # 4. 计算权重: P(Train="delayed"|Rain=rain)
    weight = probability_train_given_rain(train, rain)
    
    # 5. 将样本加入带权重的计数中
    weighted_samples[appointment] += weight
    total_weight += weight
```

最后，计算概率分布：
```python
P_attend = weighted_samples["attend"] / total_weight
P_miss = weighted_samples["miss"] / total_weight
```

## 具体数值例子

假设我们有以下概率:
- P(Rain=yes) = 0.3, P(Rain=no) = 0.7
- P(Train=delayed|Rain=yes) = 0.6, P(Train=delayed|Rain=no) = 0.1
- P(Appointment=attend|Train=delayed) = 0.6, P(Appointment=miss|Train=delayed) = 0.4

### 拒绝抽样中：
假设我们生成1000个样本:
- 约300个样本Rain=yes，其中约180个Train=delayed
- 约700个样本Rain=no，其中约70个Train=delayed
- 总共约250个样本Train=delayed (被接受)，750个样本被丢弃

### 可能性加权中：
我们生成1000个样本，每个样本都使用:
- 如果抽到Rain=yes (约300次)：权重=0.6
- 如果抽到Rain=no (约700次)：权重=0.1

所有1000个样本都被保留并计入结果，只是权重不同。

## 优势总结

1. **计算效率更高**：不浪费任何样本计算
2. **样本利用率100%**：所有生成的样本都有助于最终结果
3. **适用于稀有事件**：当证据事件概率很低时尤其有效
4. **可扩展性更好**：当有多个证据条件时，拒绝抽样的效率会急剧下降

可能性加权本质上是"不要丢弃样本，而是根据其符合证据的可能性给予权重"的思想，这在处理复杂概率模型时特别有价值。

---

# 正确答案解析

正确的句子是：**Assuming we know the train is on time, whether or not there is track maintenance does not affect the probability that the appointment is attended.**

翻译：假设我们知道火车准时，是否有轨道维护不会影响出席约会的概率。

## 为什么这个答案是正确的？

这个问题本质上是测试我们对贝叶斯网络中的**条件独立性**的理解。在贝叶斯网络中，通过查看网络结构，我们可以判断给定某些变量的情况下，其他变量之间是否独立。

让我们回顾一下这个贝叶斯网络的结构：
```
Rain → Maintenance
   ↘   ↓
     Train → Appointment
```

在这个网络中：
1. Rain（下雨）和Maintenance（轨道维护）都能影响Train（火车是否准时）
2. Train直接影响Appointment（是否参加约会）

**关键点**：根据贝叶斯网络中的条件独立性规则，**当我们已知一个节点的值时，它会"阻断"其父节点对其子节点的影响**。

因此，当我们已知Train=on_time（火车准时）时：
- Appointment变量只依赖于Train变量
- Rain和Maintenance（Train的父节点）不再对Appointment有影响

换句话说，**一旦你知道火车准时了，那么引起火车准时的原因（是否下雨、是否有轨道维护）对你能否参加约会没有任何进一步的影响**。

这符合我们的直觉：如果你只关心能否赴约，而你已经知道火车准时了，那么你不需要关心天气如何或者铁路是否在维护 - 这些因素已经通过"火车准时"这个信息被完全考虑进去了。

----


**题目与讲座概念的联系**

1.  **概率论基础:**
    *   **概率公理:** 整个问题基于概率计算，所有计算出的概率值都应在 0 和 1 之间，并且在 `normalize` 函数中，我们要确保每个分布的总和为 1。
    *   **随机变量:** 每个人的“基因数量”（可以取值 0, 1, 2）和“是否具有特征”（可以取值 True, False）都可以看作是随机变量。`PROBS` 字典定义了这些变量的（条件或无条件）概率分布。
    *   **概率分布:** `PROBS["gene"]` 是没有父母信息时基因数量的先验概率分布。`PROBS["trait"]` 是一个条件概率分布，给出在已知基因数量的情况下，表现出特征的概率 P(Trait | Gene)。`probabilities` 字典最终存储的是每个人基因数量和特征表现的后验概率分布。

2.  **条件概率:** 这是核心概念。
    *   **P(Trait | Gene):** `PROBS["trait"]` 直接给出了这个条件概率。`joint_probability` 函数在计算每个人的总概率时会用到它。
    *   **P(Child's Gene | Mother's Gene, Father's Gene):** 对于有父母的孩子，其基因数量的概率是 *条件* 依赖于其父母基因数量的。你需要根据遗传规则（从父母各随机继承一个基因）和突变概率来计算这个条件概率。这是 `joint_probability` 函数中最复杂的部分。

3.  **联合概率:**
    *   `joint_probability` 函数的**目标**就是计算一个大的联合概率：P(Person1 State ∧ Person2 State ∧ ... ∧ PersonN State)，其中每个人的 "State" 是指其特定的基因数量 *和* 特定的特征表现状态。
    *   **计算方法:** 假设（在给定父代基因的情况下）个体的基因和特征状态之间以及不同个体之间的事件是条件独立的，联合概率可以通过将每个人的个体概率（P(Gene) * P(Trait | Gene)）相乘得到。对于有父母的孩子，其 P(Gene) 本身是条件概率 P(Gene | Parents' Genes)。

4.  **贝叶斯网络:**
    *   整个家庭结构和遗传/特征关系可以被模型化为一个贝叶斯网络。
        *   **节点:** 每个人的基因数量是一个节点，每个人的特征表现是另一个节点。
        *   **边:**
            *   从父母的“基因数量”节点指向孩子的“基因数量”节点。
            *   从每个人的“基因数量”节点指向该人的“特征表现”节点。
    *   `PROBS` 字典定义了这个网络中的（先验和条件）概率表。
    *   `joint_probability` 函数实际上是在计算这个贝叶斯网络中所有节点都取特定值的一个完整联合概率 P(G1, T1, G2, T2, ...)。

5.  **推断 (Inference):**
    *   整个 `main` 函数执行的过程就是一种概率推断。
    *   **证据 (Evidence):** 输入的 `data.csv` 文件中可能包含某些人已知的特征信息（trait=1 或 trait=0）。`main` 函数在循环时会跳过与已知证据矛盾的假设 (`fails_evidence` 检查)。
    *   **查询 (Query):** 我们最终想知道的是每个人具有不同基因数量和表现出特征的概率，即 P(PersonX Gene | Evidence) 和 P(PersonX Trait | Evidence)。
    *   **枚举推断 (Inference by Enumeration):** `main` 函数通过遍历所有可能的基因分配组合 (`powerset` for `one_gene`, `two_genes`) 和特征分配组合 (`powerset` for `have_trait`)，计算每种组合的联合概率 (`joint_probability`)，然后使用 `update` 函数将这些概率累加到相应的边际分布中。这本质上是在对所有与证据兼容的“可能世界”（基因和特征的完整分配）进行求和（通过 `update` 累加）来计算我们关心的边际概率。
    *   **隐藏变量 (Hidden Variables):** 任何人的基因数量（除非在假设中明确指定）以及任何未在 CSV 中指定的特征状态，都可以看作是隐藏变量。我们通过对所有可能的隐藏状态进行求和/积分（在这里是离散求和）来计算查询变量的概率。

6.  **归一化 (Normalization):**
    *   `normalize` 函数的作用与讲座中提到的归一化因子 α 完全相同。在累加了所有可能世界的（未归一化的）联合概率之后，我们需要确保每个人的基因概率分布和特征概率分布各自加起来等于 1。这通过将每个值除以该分布的总和来实现，将累加的“计数”或“权重”转换为真正的概率。

7.  **马尔可夫模型 (间接联系):**
    *   虽然这不是一个典型的时间序列问题，但基因遗传具有“马尔可夫性质”的意味：孩子的基因状态 *只* 取决于其父母的基因状态（上一代），而不直接取决于更早的祖先（给定父母信息后）。这与马尔可夫假设（当前状态仅依赖于前一个或有限个先前状态）的思想类似。但这里的结构是树状/图状（贝叶斯网络）而非线性链。

**父母、孩子、基因、特征的关系**

1.  **基因:**
    *   每个人针对某个性状（由这个基因影响）拥有两份基因拷贝（等位基因）。
    *   这个基因可以是有功能的（我们称之为“有基因”，G）或无功能的（“无基因”，N）。
    *   因此，一个人的基因型可以是：
        *   GG（两份 G）：`two_genes`
        *   GN 或 NG（一份 G，一份 N）：`one_gene`
        *   NN（零份 G）：不在 `one_gene` 或 `two_genes` 中。
    *   **来源:**
        *   **无父母信息:** 概率由 `PROBS["gene"]` 直接给出（人群中的基础频率）。
        *   **有父母信息:** 孩子从母亲那里随机继承一份基因拷贝，从父亲那里随机继承一份。

2.  **遗传:**
    *   **母亲:**
        *   如果母亲是 GG (`two_genes`)，她总是传递 G。
        *   如果母亲是 NN (`zero_genes`)，她总是传递 N。
        *   如果母亲是 GN (`one_gene`)，她以 50% 的概率传递 G，50% 的概率传递 N。
    *   **父亲:** 同样规则。
    *   **组合:** 孩子最终的基因型（GG, GN, NN）取决于从父母双方继承的组合。例如，要得到 GN，孩子需要从一个父母那里得到 G，从另一个那里得到 N。

3.  **突变 (Mutation):**
    *   遗传不是完美的。在基因从父母传递给孩子时，有 `PROBS["mutation"]` 的小概率发生突变。
    *   **突变效果:**
        *   如果父母本应传递 G，突变可能使其变成 N。
        *   如果父母本应传递 N，突变可能使其变成 G。
    *   **计算影响:** 在计算孩子从父母那里获得 G 的概率时，必须考虑：
        *   父母传递 G 且 *未* 发生突变的概率。
        *   父母传递 N 且 *发生* 突变的概率。
        *   例如：P(母亲传递 G) = P(母亲选G)*(1-mut) + P(母亲选N)*mut

4.  **特征 (Trait):**
    *   特征是可观察的表现（例如，是否有某种疾病）。
    *   特征的 *表现概率* 取决于该个体的基因数量（GG, GN, 或 NN）。
    *   这种关系由 `PROBS["trait"]` 定义：
        *   `PROBS["trait"][2][True]` 是拥有两份基因 (GG) 的人表现出特征的概率。
        *   `PROBS["trait"][1][True]` 是拥有一份基因 (GN) 的人表现出特征的概率。
        *   `PROBS["trait"][0][True]` 是拥有零份基因 (NN) 的人表现出特征的概率。
        *   对应的 `False` 值是 *不* 表现出特征的概率（例如 `PROBS["trait"][2][False] = 1 - PROBS["trait"][2][True]`）。
    *   **关键:** 基因型 *不* 确定性地决定特征，而是影响其发生的 *概率*。

**总结:**

这个题目让你实现一个基于贝叶斯网络进行概率推断的系统，用于模拟孟德尔遗传（带有突变）和基因型到表现型（特征）的概率映射。`joint_probability` 计算特定遗传/特征组合的可能性，`update` 累积这些可能性以进行边际推断，`normalize` 确保最终结果是有效的概率分布。这与讲座中关于条件概率、联合概率、贝叶斯网络和推断（特别是枚举推断）的概念紧密相连。

---

为什么需要考虑每一种可能的全员基因状态组合？你的意思是要考虑所有人的所有基因携带情况吗？为什么这么做？？

核心原因在于概率推断的本质以及变量之间的相互依赖性。

**未知状态与间接证据:** 我们并不能直接观测到每个人的基因型。我们拥有的只是间接证据：

*   一部分人的性状表现（可能已知，可能未知）。
*   家庭结构（父母关系）。
*   已知的遗传概率（基因频率、性状表现概率、突变率）。

我们的目标是利用这些不完整的信息，来推断每个人最可能的基因状态和性状状态。

**联合概率是基础:** 单个个体的基因状态不是孤立的。一个人的基因型会受到其父母基因型的影响，同时也会影响其子女的基因型。同样，一个人的基因型会影响其表现出某种性状的概率。因此，要准确评估某个特定假设（比如“哈利有1个基因拷贝”）的可能性，我们不能只看哈利自己，而必须考虑整个系统的状态。`joint_probability` 函数计算的就是这样一个完整系统状态（即：同时确定了每个人的基因拷贝数 *和* 每个人的性状表现）发生的概率。

**边际概率的计算 (Law of Total Probability):** 我们最终关心的是单个个体的概率，比如 P(哈利有1个基因)。这被称为边际概率。根据全概率定律，要计算一个事件（哈利有1个基因）的概率，我们需要考虑所有可能导致这个事件发生的互斥场景，并将这些场景的概率加起来。

在这个问题中，“场景”就是一个完整的“全员基因状态和性状状态”组合。所以：

P(哈利有1个基因) = Σ \[ P(哈利有1个基因，并且其他人有特定的基因组合，并且所有人有特定的性状组合) ]

这个求和 Σ 需要遍历所有与“哈利有1个基因”这个条件相容的全员状态组合。

**代码实现:**

*   嵌套的 `powerset` 循环 (`for have_trait`, `for one_gene`, `for two_genes`) 正是在系统性地生成每一个可能的全员状态组合。
*   `joint_probability` 计算当前这个特定组合发生的概率 `p`。
*   `update` 函数将这个概率 `p` 累加到该组合中涉及的每个人的相应边际概率计数器上。例如，如果当前组合是 {哈利:1基因, 赫敏:0基因, ... , 哈利:有性状, 赫敏:无性状, ...}，那么 `p` 就会被加到 `probabilities[哈利]["gene"][1]`、`probabilities[赫敏]["gene"][0]`、`probabilities[哈利]["trait"][True]`、`probabilities[赫敏]["trait"][False]` 等等上面。
*   当所有组合都遍历并累加完毕后，`normalize` 函数再将这些累加值转换成最终的概率分布。

**简单来说:** 因为每个人的基因和性状都可能相互影响（通过遗传关系），我们不能孤立地看待任何一个人。为了得到关于某个人的准确概率推断，我们必须考虑所有可能的“世界状态”（即全员的基因和性状组合），计算每种状态的可能性，然后把与我们关心的事件（如“哈利有1个基因”）相关的可能性加总起来。

---

虽然这个遗传项目和隐马尔可夫模型（Hidden Markov Model, HMM）有一些相似之处，但将它直接归类为 HMM 的实践可能不太准确，它更精确地属于贝叶斯网络（Bayesian Network） 的范畴。

让我们比较一下：

**相似之处:**

*   **隐藏状态 (Hidden States):** 每个人的基因型（0, 1, 或 2 个基因拷贝）是未知的，可以看作是隐藏状态。
*   **观测值 (Observations/Emissions):** 每个人的性状表现（有或无）是可观测的（或部分可观测），其概率依赖于隐藏的基因型状态。这类似于 HMM 中的发射概率。
*   **概率推断:** 两者都旨在根据观测值来推断隐藏状态的概率。

**关键区别:**

*   **模型结构:**
    *   **HMM:** 通常用于建模序列数据（如时间序列、语音信号、文本序列）。其核心是状态之间的顺序转移概率（从时间 t 的状态转移到时间 t+1 的状态）。模型结构是一条链。
    *   **遗传项目:** 建模的是家庭成员之间的关系，形成一个树状或图状结构（谱系图/Pedigree）。依赖关系不是线性的时间顺序，而是基于亲子遗传。一个孩子的基因型概率取决于其父母的基因型概率。这种结构更符合贝叶斯网络的定义。
*   **依赖关系:**
    *   **HMM:** 状态转移通常满足马尔可夫性质，即下一个状态只依赖于当前状态。观测值只依赖于当前状态。
    *   **遗传项目:** 一个人的基因型依赖于其父母双方的基因型。一个人的性状表现依赖于其自身的基因型。这种复杂的、非线性的依赖关系网络是贝叶斯网络的特征。
*   **推断算法:**
    *   **HMM:** 有专门的高效算法，如前向-后向算法（Forward-Backward Algorithm）用于计算状态概率，维特比算法（Viterbi Algorithm）用于找到最可能的状态序列。这些算法利用了模型的线性链结构。
    *   **遗传项目 (此代码实现):** 采用的是精确推断中的变量消除（Variable Elimination）或枚举所有联合概率的方法。它通过遍历所有可能的隐藏状态组合（全员基因型）来计算边际概率。对于更复杂的谱系图，这种方法计算量会爆炸式增长，通常需要使用贝叶斯网络的其他推断算法（如连接树算法或近似推断方法如 MCMC/Gibbs 采样）。

**总结:**

虽然遗传问题涉及隐藏状态（基因型）和依赖于这些状态的观测值（性状），这与 HMM 的核心思想一致，但其底层的依赖结构是一个基于家庭关系的网络（贝叶斯网络），而不是 HMM 的线性链。

因此，这个项目更准确地说是应用贝叶斯网络进行概率推断的一个实例，而不是一个典型的 HMM 实践。它让你体验了如何在一个具有特定图结构（谱系图）的概率模型中，通过考虑所有可能性来计算边际概率。