# CS50 人工智能 - Lecture 4 笔记

来源: https://cs50.harvard.edu/ai/2024/notes/4/

## 第四讲

### 机器学习

机器学习为计算机提供数据，而不是明确的指令。使用这些数据，计算机学习识别模式并能够自行执行任务。

### 监督学习

监督学习是一项任务，其中计算机基于输入-输出对的数据集学习将输入映射到输出的函数。

监督学习下有多个任务，其中之一是分类。这是一项函数将输入映射到离散输出的任务。例如，给定某一天湿度和气压的一些信息（输入），计算机决定那天是否会下雨（输出）。计算机在已经将湿度和气压映射到是否下雨的多个日期的数据集上进行训练后执行此操作。

此任务可以形式化如下。我们观察自然界，其中函数 f(湿度, 气压) 将输入映射到离散值，即下雨或不下雨。此函数对我们隐藏，并且可能受到我们无法访问的许多其他变量的影响。我们的目标是创建函数 h(湿度, 气压)，它可以近似函数 f 的行为。可以通过在湿度和降雨（输入）维度上绘制日期来可视化此类任务，如果那天下雨，则将每个数据点涂成蓝色，如果那天不下雨，则涂成红色（输出）。白色数据点只有输入，计算机需要找出输出。

[湿度和气压与降雨量图]

## 最近邻分类

解决上述任务的一种方法是将变量的值分配给最近观察的值。因此，例如，上图中的白点将被涂成蓝色，因为最近的观察点也是蓝色。这有时可能效果很好，但请考虑下图。

[另一个湿度和气压与降雨量图]

按照相同的策略，白点应涂成红色，因为离它最近的观察结果也是红色。但是，从更大的范围来看，似乎它周围的大多数其他观察结果都是蓝色，这可能会给我们直觉，在这种情况下蓝色是更好的预测，即使最近的观察结果是红色。

解决最近邻分类局限性的一种方法是使用 k-最近邻分类，其中点的颜色基于 k 个最近邻居中最常见的颜色进行着色。k 的值由程序员决定。例如，使用 3-最近邻分类，上面的白点将被涂成蓝色，这在直觉上似乎是一个更好的决定。

k-最近邻分类的一个缺点是，使用朴素方法，算法将不得不测量每个点到相关点的距离，这在计算上是昂贵的。可以通过使用能够更快找到邻居的数据结构或通过修剪不相关的观察结果来加快速度。

## 感知器学习

与最近邻策略相反，解决分类问题的另一种方法是将数据作为一个整体来查看，并尝试创建决策边界。在二维数据中，我们可以在两种类型的观察结果之间画一条线。每个附加数据点将根据其绘制在直线的哪一侧进行分类。

[决策边界图]

这种方法的缺点是数据是混乱的，并且很少能画一条线并将类整齐地分为两个观察结果而没有任何错误。通常，我们会妥协，绘制一个边界，该边界可以正确地分隔观察结果，但仍然偶尔会错误分类它们。

在这种情况下，

- x₁ = 湿度
- x₂ = 气压

的输入将提供给假设函数 h(x₁, x₂)，该函数将输出其对当天是否会下雨的预测。它将通过检查观察结果落在决策边界的哪一侧来做到这一点。形式上，该函数将使用常数来权衡每个输入，最终得到以下形式的线性方程：

- 下雨 w₀ + w₁x₁ + w₂x₂ ≥ 0
- 否则不下雨

通常，输出变量将被编码为 1 和 0，其中如果方程产生的值大于 0，则输出为 1（下雨），否则为 0（不下雨）。

权重和值由向量表示，向量是数字序列（可以存储在 Python 中的列表或元组中）。我们生成权重向量 w：（w₀, w₁, w₂），获得最佳权重向量是机器学习算法的目标。我们还生成输入向量 x：（1, x₁, x₂）。

我们取两个向量的点积。也就是说，我们将一个向量中的每个值乘以第二个向量中的对应值，得到上面的表达式：w₀ + w₁x₁ + w₂x₂。输入向量中的第一个值是 1，因为当乘以权重向量 w₀ 时，我们希望保持它不变。

因此，我们可以通过以下方式表示我们的假设函数：

[假设函数公式]

由于算法的目标是找到最佳权重向量，因此当算法遇到新数据时，它会更新当前权重。它使用感知器学习规则来做到这一点：

[感知器学习规则公式]

从该规则中获得的重要启示是，对于每个数据点，我们都会调整权重以使我们的函数更准确。不太重要的细节是，每个权重都设置为等于自身加上括号中的某个值。在这里，y 代表观察值，而假设函数代表估计值。如果它们相同，则整个项等于零，因此权重不会更改。如果我们低估了（在观察到下雨时称为不下雨），则括号中的值将为 1，并且权重将按 xᵢ 的值缩放学习系数 α 而增加。如果我们高估了（在观察到不下雨时称为下雨），则括号中的值将为 -1，并且权重将按 x 的值缩放 α 而减小。α 越高，每个新事件对权重的影响就越大。

此过程的结果是一个阈值函数，一旦估计值超过某个阈值，该阈值函数就会从 0 切换到 1。

[阈值函数图]

这种类型函数的问题在于它无法表达不确定性，因为它只能等于 0 或 1。它采用硬阈值。解决此问题的一种方法是使用 logistic 函数，该函数采用软阈值。Logistic 函数可以产生 0 到 1 之间的实数，这将表达对估计值的置信度。值越接近 1，下雨的可能性就越大。

## 支持向量机

除了最近邻和线性回归之外，分类的另一种方法是支持向量机。这种方法使用决策边界附近的附加向量（支持向量）在分隔数据时做出最佳决策。考虑下面的例子。

[支持向量机图]

所有决策边界都有效，因为它们在没有任何错误的情况下分隔数据。但是，它们同样好吗？最左边的两个决策边界非常接近某些观察结果。这意味着仅与一组略有不同的新数据点可能会被错误地分类为另一组。与此相反，最右边的决策边界与它分隔的两个组保持最远的距离，从而为组内的变化提供了最大的回旋余地。这种类型的边界尽可能远离它分隔的两个组，称为最大边距分隔符。

支持向量机的另一个好处是，它们可以表示具有两个以上维度的决策边界，以及非线性决策边界，如下所示。

[非线性决策边界图]

总而言之，有多种方法可以解决分类问题，没有一种方法总是比另一种方法更好。每种方法都有其缺点，并且在特定情况下可能比其他方法更有用。

## 回归

回归是监督学习任务，其函数将输入点映射到连续值，即某个实数。这与分类不同，因为分类问题将输入映射到离散值（下雨或不下雨）。

例如，公司可能会使用回归来回答广告支出如何预测销售收入的问题。在这种情况下，观察到的函数 f(广告) 表示在广告中花费一定金额后观察到的收入（请注意，该函数可以采用多个输入变量）。这些是我们开始使用的数据。有了这些数据，我们想要提出一个假设函数 h(广告)，它将尝试近似函数 f 的行为。h 将生成一条线，其目标不是在观察结果类型之间进行分隔，而是根据输入来预测输出值。

[回归线图]

## 损失函数

损失函数是一种量化上述任何决策规则所损失效用的方法。预测越不准确，损失越大。

对于分类问题，我们可以使用 0-1 损失函数。

- L(实际值, 预测值):
  - 如果 实际值 = 预测值 则为 0
  - 否则为 1

用文字表达，当预测不正确时，此函数会获得值，而当预测正确时（即当观察值和预测值匹配时），则不会获得值。

[0-1 损失函数图]

在上面的示例中，值为 0 的日期是我们正确预测天气的日期（雨天在直线下方，而非雨天在直线上方）。但是，不下雨的日期在直线下方，而下雨的日期在直线上方，这些是我们未能预测的日期。我们为每个日期赋值 1 并将它们加起来，以获得对我们的决策边界损失程度的经验估计。

当预测连续值时，可以使用 L₁ 和 L₂ 损失函数。在这种情况下，我们有兴趣量化每个预测与观察值的差异程度。我们通过取观察值减去预测值的绝对值或平方值来做到这一点（即预测值与观察值的距离）。

- L₁: L(实际值, 预测值) = |实际值 - 预测值|
- L₂: L(实际值, 预测值) = (实际值 - 预测值)²

可以选择最能满足其目标的损失函数。L₂ 比 L₁ 更严厉地惩罚异常值，因为它对差异进行平方。L₁ 可以通过将每个观察点到回归线上预测点的距离相加来可视化：

[L1 损失函数图]

## 过拟合

过拟合是指模型非常适合训练数据，以至于无法推广到其他数据集。从这个意义上讲，损失函数是一把双刃剑。在下面的两个示例中，损失函数被最小化，使得损失等于 0。但是，它不太可能很好地拟合新数据。

[过拟合模型图]

例如，在左图中，屏幕底部红色点旁边的点很可能是下雨（蓝色）。但是，使用过拟合模型，它将被分类为不下雨（红色）。

## 正则化

正则化是惩罚更复杂的假设以支持更简单、更通用的假设的过程。我们使用正则化来避免过拟合。

在正则化中，我们通过将假设函数 h 的损失及其复杂性度量相加来估计假设函数 h 的成本。

成本(h) = 损失(h) + λ \* 复杂性(h)

Lambda (λ) 是一个常数，我们可以使用它来调节成本函数中对复杂性的惩罚强度。λ 越高，复杂性成本越高。

测试我们是否过度拟合模型的一种方法是使用留出交叉验证。在此技术中，我们将所有数据分为两部分：训练集和测试集。我们在训练集上运行学习算法，然后查看它在测试集中预测数据的效果如何。这里的想法是通过在未用于训练的数据上进行测试，我们可以衡量学习的泛化程度。

留出交叉验证的缺点是我们无法在半数数据上训练模型，因为它用于评估目的。解决此问题的一种方法是使用 k 折交叉验证。在此过程中，我们将数据分为 k 个集合。我们运行训练 k 次，每次都留出一个数据集并将其用作测试集。我们最终得到 k 个不同的模型评估，我们可以对其进行平均并获得对模型泛化程度的估计，而不会丢失任何数据。

## scikit-learn

与 Python 的常见情况一样，有多个库允许我们方便地使用机器学习算法。scikit-learn 就是其中之一。

例如，我们将使用伪造纸币的 CSV 数据集。

[伪造纸币数据集]

左侧的四列是我们可以用来预测钞票是真钞还是伪钞的数据，这是由人提供的外部数据，编码为 0 和 1。现在我们可以在此数据集上训练我们的模型，看看我们是否可以预测新钞票是真钞还是伪钞。

```python
import csv
import random

from sklearn import svm
from sklearn.linear_model import Perceptron
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier

# model = KNeighborsClassifier(n_neighbors=1)
# model = svm.SVC()
model = Perceptron()

# 从文件中读取数据
with open("banknotes.csv") as f:
    reader = csv.reader(f)
    next(reader)

    data = []
    for row in reader:
        data.append({
            "evidence": [float(cell) for cell in row[:4]],
            "label": "Authentic" if row[4] == "0" else "Counterfeit"
        })

# 将数据分为训练组和测试组
holdout = int(0.40 * len(data))
random.shuffle(data)
testing = data[:holdout]
training = data[holdout:]

# 在训练集上训练模型
X_training = [row["evidence"] for row in training]
y_training = [row["label"] for row in training]
model.fit(X_training, y_training)

# 对测试集进行预测
X_testing = [row["evidence"] for row in testing]
y_testing = [row["label"] for row in testing]
predictions = model.predict(X_testing)

# 计算我们的表现
correct = 0
incorrect = 0
total = 0
for actual, predicted in zip(y_testing, predictions):
    total += 1
    if actual == predicted:
        correct += 1
    else:
        incorrect += 1

# 打印结果
print(f"模型 {type(model).__name__} 的结果")
print(f"正确: {correct}")
print(f"不正确: {incorrect}")
print(f"准确率: {100 * correct / total:.2f}%")
```

可以在本讲义的源代码部分下的 banknotes0.py 中找到此手动运行算法的版本。由于该算法经常以类似的方式使用，因此 scikit-learn 包含其他功能，这些功能使代码更加简洁易用，并且可以在 banknotes1.py 下找到此版本。

## 强化学习

强化学习是机器学习的另一种方法，其中在每次操作之后，代理都会获得奖励或惩罚形式的反馈（正数或负数值）。

[强化学习循环图]

学习过程从环境向代理提供状态开始。然后，代理对状态执行操作。基于此操作，环境将向代理返回状态和奖励，其中奖励可以是正数（使未来的行为更有可能发生）或负数（即惩罚）（使未来的行为不太可能发生）。

例如，这种类型的算法可用于训练步行机器人，其中每一步返回一个正数（奖励），每次跌倒返回一个负数（惩罚）。

## 马尔可夫决策过程

强化学习可以看作是马尔可夫决策过程，具有以下属性：

- 状态集 S
- 动作集 Actions(S)
- 转移模型 P(s' | s, a)
- 奖励函数 R(s, a, s')

例如，考虑以下任务：

[网格世界图]

代理是黄色圆圈，它需要到达绿色方块，同时避开红色方块。任务中的每个方块都是一个状态。向上、向下或侧向移动是一种动作。转移模型为我们提供了执行动作后的新状态，奖励函数是代理获得的反馈类型。例如，如果代理选择向右走，它将踩到红色方块并获得负反馈。这意味着代理将学习到，当处于左下角方块的状态时，它应该避免向右走。这样，代理将开始探索空间，学习它应该避免哪些状态-动作对。该算法可以是概率性的，基于奖励增加或减少的概率，在不同状态下选择采取不同的操作。当代理到达绿色方块时，它将获得正奖励，从而学习到它在上一个状态中采取的行动是有利的。

## Q-学习

Q-学习是一种强化学习模型，其中函数 Q(s, a) 输出在状态 s 中采取行动 a 的价值估计。

该模型从所有估计值等于 0 开始（对于所有 s, a，Q(s,a) = 0）。当采取行动并收到奖励时，该函数会执行两项操作：1) 它基于当前奖励和预期的未来奖励来估计 Q(s, a) 的值，以及 2) 更新 Q(s, a) 以同时考虑旧估计值和新估计值。这为我们提供了一种算法，该算法能够改进其过去的知识，而无需从头开始。

[Q-学习更新规则公式]

Q(s, a) 的更新值等于 Q(s, a) 的先前值加上一些更新值。此值确定为新值和旧值之间的差值，乘以学习系数 α。当 α = 1 时，新估计值只是覆盖旧估计值。当 α = 0 时，估计值永远不会更新。通过升高和降低 α，我们可以确定先前知识被新估计值更新的速度。

新值估计可以表示为奖励 (r) 和未来奖励估计的总和。为了获得未来奖励估计，我们考虑在采取最后一个行动后获得的新状态，并添加在此新状态下将带来最高奖励的行动估计。这样，我们不仅通过收到的奖励来估计在状态 s 中执行动作 a 的效用，还通过下一步的预期效用进行估计。未来奖励估计的值有时可能会以系数 gamma 出现，该系数控制未来奖励的价值。我们最终得到以下公式：

[新值估计公式]

贪婪决策算法完全忽略了未来的估计奖励，而是始终选择在当前状态 s 中具有最高 Q(s, a) 的动作 a。

这使我们讨论了探索与利用的权衡。贪婪算法始终利用，采取已建立的行动以带来良好的结果。但是，它将始终遵循相同的解决方案路径，永远找不到更好的路径。另一方面，探索意味着该算法可能会在其通往目标的途中使用以前未探索的路线，从而使其能够沿途发现更有效的解决方案。例如，如果您每次都听相同的歌曲，您知道您会喜欢它们，但您永远不会了解您可能更喜欢的新歌曲！

为了实现探索和利用的概念，我们可以使用 ε（epsilon）贪婪算法。在这种类型的算法中，我们将 ε 设置为我们想要随机移动的频率。在概率为 1-ε 的情况下，算法选择最佳移动（利用）。在概率为 ε 的情况下，算法选择随机移动（探索）。

训练强化学习模型的另一种方法是在整个过程结束后而不是在每次移动时给出反馈。例如，考虑尼姆游戏。在此游戏中，不同数量的物体分布在各个堆中。每个玩家从任何一个堆中取出任意数量的物体，而取走最后一个物体的玩家输掉。在这样的游戏中，未经训练的 AI 将随机玩游戏，并且很容易战胜它。为了训练 AI，它将从随机玩游戏开始，并在最后获得 1 的奖励（获胜）和 -1 的奖励（失败）。例如，当它接受 10,000 场游戏的训练时，它已经足够聪明，难以战胜。

当游戏具有多种状态和可能的动作（例如国际象棋）时，此方法在计算上变得更加苛刻。为每种可能状态下的每种可能移动生成估计值是不可行的。在这种情况下，我们可以使用函数逼近，这使我们能够使用各种其他特征来近似 Q(s, a)，而不是为每个状态-动作对存储一个值。因此，该算法变得能够识别哪些移动足够相似，以至于它们的估计值也应该相似，并在其决策中使用此启发式方法。

## 无监督学习

在我们之前看到的所有案例中，与监督学习一样，我们都有带有标签的数据，算法可以从中学习。例如，当我们训练算法来识别伪造钞票时，每张钞票都有四个变量，它们具有不同的值（输入数据）以及它是伪造的还是真实的（标签）。在无监督学习中，仅存在输入数据，而 AI 会学习这些数据中的模式。

### 聚类

聚类是一种无监督学习任务，它获取输入数据并将其组织成组，以便相似的对象最终位于同一组中。例如，这可以用于遗传学研究（在尝试查找相似基因时）或图像分割（在基于像素之间的相似性定义图像的不同部分时）。

## k-均值聚类

k-均值聚类是一种执行聚类任务的算法。它将所有数据点映射到一个空间中，然后在该空间中随机放置 k 个聚类中心（由程序员决定数量；这是我们在左侧看到的起始状态）。每个聚类中心只是空间中的一个点。然后，每个聚类都会分配所有点，这些点比任何其他中心都更接近其中心（这是中间图片）。然后，在迭代过程中，聚类中心移动到所有这些点的中间（右侧的状态），然后再次将点重新分配给现在离它们最近的中心所在的聚类。当在重复该过程后，每个点都保持在之前的聚类中时，我们就达到了平衡，算法结束，从而使点在聚类之间分开。

[k-均值聚类图]
