# CS50 AI - 第 6 讲笔记

这些笔记反映了 2023 年 8 月 14 日发布的第 6 讲新版本。如果您观看了之前版本的讲座并希望查看其笔记，请点击此处。

## 语言

到目前为止在课程中，我们需要塑造任务和数据，使 AI 能够处理它们。今天，我们将看看如何构建 AI 来处理人类语言。

自然语言处理涵盖了 AI 以人类语言为输入的所有任务。以下是此类任务的一些例子：

- 自动摘要，AI 接收文本作为输入，并生成文本摘要作为输出。
- 信息提取，AI 接收一个文本语料库，并提取数据作为输出。
- 语言识别，AI 接收文本并返回文本的语言作为输出。
- 机器翻译，AI 接收原始语言的文本，并输出目标语言的翻译。
- 命名实体识别，AI 接收文本并提取文本中实体的名称（例如，公司名称）。
- 语音识别，AI 接收语音并以文本形式生成相同的词语。
- 文本分类，AI 接收文本并需要将其分类为某种类型的文本。
- 词义消歧，AI 需要为具有多种含义的词选择正确的含义（例如，bank 既表示金融机构，也表示河岸）。

## 语法和语义

语法是句子结构。作为某种人类语言的母语使用者，我们不会在生成语法正确的句子和将不合语法的句子标记为错误方面遇到困难。例如，"就在九点钟之前，夏洛克·福尔摩斯快步走进房间"是符合语法的，而"就在夏洛克·福尔摩斯九点钟快步走进房间"是不符合语法的。语法可以同时是正确的和模糊的，如"我用望远镜看到了那个男人。"是我看到（带着望远镜的男人）还是我（看到了那个男人），是通过望远镜看到的？为了能够解析人类语言并生成它，AI 需要掌握语法。

语义是词语或句子的含义。虽然句子"就在九点钟之前，夏洛克·福尔摩斯快步走进房间"在语法上与"夏洛克·福尔摩斯在九点钟之前快步走进房间"不同，但它们的内容实际上是相同的。同样，尽管句子"九点前几分钟，夏洛克·福尔摩斯快速地走进房间"使用了与之前句子不同的词，但它仍然传达着非常相似的含义。此外，一个句子可以在语法上完全正确，同时又完全没有意义，就像乔姆斯基的例子，"无色的绿色想法猛烈地睡觉。"为了能够解析人类语言并生成它，AI 需要掌握语义。

## 上下文无关文法

形式语法是一个用于在语言中生成句子的规则系统。在上下文无关文法中，文本从其含义中抽象出来，使用形式语法表示句子的结构。让我们考虑以下示例句子：

- She saw the city.（她看见了这座城市。）

这是一个简单的语法正确的句子，我们希望生成一个表示其结构的语法树。

我们首先为每个词分配其词性。She 和 city 是名词，我们将其标记为 N。Saw 是动词，我们将其标记为 V。The 是限定词，将后面的名词标记为特定或不特定，我们将其标记为 D。现在，上述句子可以重写为：

- N V D N

到目前为止，我们已经将每个词从其语义含义抽象为其词性。然而，句子中的词彼此相连，要理解句子，我们必须理解它们如何连接。名词短语（NP）是连接到名词的一组词。例如，she 这个词在这个句子中是一个名词短语。此外，the city 这些词也形成一个名词短语，由一个限定词和一个名词组成。动词短语（VP）是连接到动词的一组词。saw 这个词本身就是一个动词短语。然而，saw the city 这些词也构成一个动词短语。在这种情况下，它是一个由动词和名词短语组成的动词短语，而这个名词短语又由限定词和名词组成。最后，整个句子（S）可以表示如下：

使用形式语法，AI 能够表示句子的结构。在我们描述的语法中，有足够的规则来表示上述简单句子。为了表示更复杂的句子，我们需要在形式语法中添加更多规则。

## nltk

与 Python 中通常的情况一样，已经编写了多个库来实现上述想法。nltk（自然语言工具包）就是这样一个库。为了分析上面的句子，我们将为算法提供语法规则：

```python
import nltk

grammar = nltk.CFG.fromstring("""
    S -> NP VP

    NP -> D N | N
    VP -> V | V NP

    D -> "the" | "a"
    N -> "she" | "city" | "car"
    V -> "saw" | "walked"
""")

parser = nltk.ChartParser(grammar)
```

类似于我们上面所做的，我们定义了可能包含在其他组件中的组件。一个句子可以包含一个名词短语和一个动词短语，而这些短语本身可以由其他短语、名词、动词等组成，最后，每个词性包含语言中的一些词。

```python
sentence = input("Sentence: ").split()
try:
    for tree in parser.parse(sentence):
        tree.pretty_print()
        tree.draw()
except ValueError:
    print("No parse tree possible.")
```

在给算法输入一个拆分成单词列表的句子后，该函数打印生成的语法树（pretty_print）并生成图形表示（draw）...

## n-grams

n-gram 是从文本样本中提取的 n 个项目的序列。在字符 n-gram 中，项目是字符，而在词 n-gram 中，项目是词。一元语法、二元语法和三元语法分别是一个、两个和三个项目的序列。在以下句子中，前三个 n-grams 是"how often have"，"often have I"和"have I said"。

"How often have I said to you that when you have eliminated the impossible whatever remains, however improbable, must be the truth?"

n-grams 对文本处理很有用。虽然 AI 不一定见过整个句子，但它肯定见过其中的部分，如"have I said"。由于某些词比其他词更频繁地一起出现，也可以以某种概率预测下一个词。例如，你的智能手机根据你输入的最后几个词的概率分布来建议词。因此，自然语言处理的一个有用步骤是将句子分解为 n-grams。

## 分词

分词是将字符序列分割成片段（标记）的任务。标记可以是词，也可以是句子，在这种情况下，任务被称为词分词或句子分词。我们需要分词才能查看 n-grams，因为它们依赖于标记序列。我们首先根据空格字符将文本分割成词。虽然这是一个好的开始，但这种方法并不完美，因为我们最终得到的词带有标点符号，例如"remains,"。因此，我们可以删除标点符号。然而，我们随后面临额外的挑战，例如带有撇号的词（例如"o'clock"）和连字符（例如"pearl-grey"）。此外，一些标点符号对句子结构很重要，如句号。但是，我们需要能够区分"Mr."一词末尾的句号和句子末尾的句号。处理这些问题就是分词过程。最后，一旦我们有了标记，我们就可以开始查看 n-grams。

## 马尔可夫模型

如前几讲所述，马尔可夫模型由节点组成，每个节点的值基于有限数量的前一个节点具有概率分布。马尔可夫模型可以用来生成文本。为此，我们在文本上训练模型，然后根据 n-gram 中前面的 n 个词为第 n 个标记建立概率分布。例如，使用三元语法，马尔可夫模型有两个词后，可以从基于前两个词的概率分布中选择第三个词。然后，它可以从基于第二和第三个词的概率分布中选择第四个词。要查看使用 nltk 实现此类模型的示例，请参考源代码中的 generator.py，其中我们的模型学习生成类似莎士比亚风格的句子。最终，使用马尔可夫模型，我们能够生成通常在语法上正确且表面上类似于人类语言输出的文本。然而，这些句子缺乏实际意义和目的。

## 词袋模型

词袋是一种将文本表示为无序词集合的模型。该模型忽略语法，仅考虑句子中词的含义。这种方法在某些分类任务中很有帮助，例如情感分析（另一种分类任务是区分普通电子邮件和垃圾电子邮件）。情感分析可用于例如产品评论，将评论分类为积极或消极。考虑以下句子：

- "My grandson loved it! So much fun!"（我的孙子喜欢它！太有趣了！）
- "Product broke after a few days."（产品几天后就坏了。）
- "One of the best games I've played in a long time."（这是我长时间以来玩过的最好的游戏之一。）
- "Kind of cheap and flimsy, not worth it."（有点便宜和脆弱，不值得。）

仅基于每个句子中的词并忽略语法，我们可以看到句子 1 和 3 是积极的（"loved"，"fun"，"best"），而句子 2 和 4 是消极的（"broke"，"cheap"，"flimsy"）。

## 朴素贝叶斯

朴素贝叶斯是一种可以在词袋模型中用于情感分析的技术。在情感分析中，我们询问"给定句子中的词，句子是积极/消极的概率是多少？"回答这个问题需要计算条件概率，回顾讲座 2 中的贝叶斯规则...

现在，我们想使用这个公式来找到 P(sentiment | text)，例如，P(positive | "my grandson loved it")。我们首先对输入进行分词，使得我们得到 P(positive | "my", "grandson", "loved", "it")。直接应用贝叶斯规则，我们得到以下表达式：P("my", "grandson", "loved", "it" | positive)\*P(positive)/P("my", "grandson", "loved", "it")。这个复杂的表达式将给我们 P(positive | "my", "grandson", "loved", "it")的精确答案。

然而，如果我们愿意得到一个不等于，但与 P(positive | "my", "grandson", "loved", "it")成比例的答案，我们可以简化表达式。稍后，知道概率分布需要加起来等于 1，我们可以将得到的值归一化为精确的概率。这意味着我们可以将上述表达式简化为仅分子：P("my", "grandson", "loved", "it" | positive)*P(positive)。同样，我们可以基于给定 b 的 a 的条件概率与 a 和 b 的联合概率成比例的知识来简化这个表达式。因此，我们得到以下概率表达式：P(positive, "my", "grandson", "loved", "it")*P(positive)。但是，计算这种联合概率很复杂，因为每个词的概率都取决于前面词的概率。这需要我们计算 P(positive)*P("my" | positive)*P("grandson" | positive, "my")*P(loved | positive, "my", "grandson")*P("it" | positive, "my", "grandson", "loved")。

这就是我们使用贝叶斯规则时的朴素之处：我们假设每个词的概率与其他词无关。这不是真的，但尽管不精确，朴素贝叶斯仍能产生良好的情感估计。使用这个假设，我们最终得到以下概率：P(positive)*P("my" | positive)*P("grandson" | positive)*P("loved" | positive)*P("it" | positive)，这不难计算。P(positive) = 所有积极样本数除以总样本数。P("loved" | positive)等于带有"loved"一词的积极样本数除以积极样本数。让我们考虑下面的例子，笑脸和皱眉表情符号代替了"positive"和"negative"词...

右边我们看到一个表格，显示了左边每个词在句子中出现的条件概率，前提是句子是积极或消极的。左侧的小表格显示了句子为积极或消极的概率。左下方我们看到根据计算得出的结果概率。此时，它们彼此成比例，但在概率方面并不能告诉我们太多。为了得到概率，我们需要归一化这些值，得到 P(positive) = 0.6837 和 P(negative) = 0.3163。朴素贝叶斯的优势在于它对在一种句子中比在另一种句子中更频繁出现的词很敏感。在我们的例子中，"loved"一词在积极句子中出现得比在消极句子中更频繁，这使得整个句子更可能是积极的而不是消极的。要查看使用 nltk 库的朴素贝叶斯进行情感评估的实现，请参考 sentiment.py。

我们可能遇到的一个问题是，某些词可能在某种类型的句子中从未出现过。假设我们样本中的积极句子都没有"grandson"一词。那么，P("grandson" | positive) = 0，当计算句子为积极的概率时，我们将得到 0。然而，在现实中并非如此（不是所有提到孙子的句子都是消极的）。解决这个问题的一种方法是加法平滑，即在我们的分布中为每个值添加一个值 α 来平滑数据。这样，即使某个值为 0，通过添加 α，我们也不会将积极或消极句子的整个概率乘以 0。一种特定类型的加法平滑，拉普拉斯平滑在我们的分布中为每个值添加 1，假装所有值至少被观察到一次。

## 词表示

我们希望在 AI 中表示词的含义。如前所见，以数字形式向 AI 提供输入是很方便的。一种方法是使用独热表示，其中每个词都用一个向量表示，该向量包含与我们拥有的词一样多的值。除了向量中等于 1 的一个值外，所有其他值都等于 0。我们如何区分词是通过哪个值为 1，最终每个词都有一个唯一的向量。例如，句子"He wrote a book"可以表示为四个向量：

-[1] (he) -[1] (wrote) -[1] (a) -[1] (book)

然而，虽然这种表示在只有四个词的世界中有效，但如果我们想表示字典中的词，当我们有 50,000 个词时，我们最终会得到 50,000 个长度为 50,000 的向量。这非常低效。这种表示的另一个问题是，我们无法表示"wrote"和"authored"等词之间的相似性。相反，我们转向分布式表示的想法，其中含义分布在向量的多个值中。使用分布式表示，每个向量都有有限数量的值（远远少于 50,000），采用以下形式：

- [-0.34, -0.08, 0.02, -0.18, …] (he)
- [-0.27, 0.40, 0.00, -0.65, …] (wrote)
- [-0.12, -0.25, 0.29, -0.09, …] (a)
- [-0.23, -0.16, -0.05, -0.57, …] (book)

这使我们能够为每个词生成唯一的值，同时使用更小的向量。此外，现在## 词表示

我们希望在 AI 中表示词的含义。如前所见，以数字形式向 AI 提供输入是很方便的。一种方法是使用独热表示，其中每个词都用一个向量表示，该向量包含与我们拥有的词一样多的值。除了向量中等于 1 的一个值外，所有其他值都等于 0。我们如何区分词是通过哪个值为 1，最终每个词都有一个唯一的向量。例如，句子"He wrote a book"可以表示为四个向量：

-[1] (he)

- [0, 1,ote)
- [0, 0, 1,1] (book)

然而，虽然这种表示在只有四个词的世界中有效，但如果我们想表示字典中的词，当我们有 50,000 个词时，我们最终会得到 50,000 个长度为 50,000 的向量。这非常低效。这种表示的另一个问题是，我们无法表示"wrote"和"authored"等词之间的相似性。相反，我们转向分布式表示的想法，其中含义分布在向量的多个值中。使用分布式表示，每个向量都有有限数量的值（远远少于 50,000），采用以下形式：

- [-0.34, -0.08, 0.02, -0.18, …] (he)
- [-0.27, 0.40, 0.00, -0.65, …] (wrote)
- [-0.12, -0.25, 0.29, -0.09, …] (a)
- [-0.23, -0.16, -0.05, -0.57, …] (book)

这使我们能够为每个词生成唯一的值，同时使用更小的向量。此外，现在我们能够通过其向量值的差异来表示词之间的相似性。

"You shall know a word by the company it keeps"（你可以通过一个词所处的环境来了解它）是英国语言学家 J.R. Firth 的观点。按照这一思想，我们可以通过相邻词来定义词。例如，在句子"for \_\_\_ he ate."中，能填入空格的词是有限的。这些词可能是"breakfast"、"lunch"和"dinner"等。这让我们得出结论，通过考虑某个词倾向于出现的环境，我们可以推断该词的含义。

## word2vec

word2vec 是一种生成词分布式表示的算法。它通过 Skip-Gram 架构实现，这是一种用于预测给定目标词的上下文的神经网络架构。在这种架构中，神经网络为每个目标词提供一个输入单元。一个较小的单一隐藏层（例如 50 或 100 个单元，这个数字是灵活的）将生成代表词分布式表示的值。这个隐藏层的每个单元都与输入层的每个单元相连。输出层将生成可能与目标词出现在相似上下文中的词。与上一讲所见类似，这个网络需要使用反向传播算法通过训练数据集进行训练。

这个神经网络非常强大。在处理结束时，每个词最终都只是一个向量或一系列数字。例如：

book: [-0.226776 -0.155999 -0.048995 -0.569774 0.053220 0.124401 -0.091108 -0.606255 -0.114630 0.473384 0.061061 0.551323 -0.245151 -0.014248 -0.210003 0.316162 0.340426 0.232053 0.386477 -0.025104 -0.024492 0.342590 0.205586 -0.554390 -0.037832 -0.212766 -0.048781 -0.088652 0.042722 0.000270 0.356324 0.212374 -0.188433 0.196112 -0.223294 -0.014591 0.067874 -0.448922 -0.290960 -0.036474 -0.148416 0.448422 0.016454 0.071613 -0.078306 0.035400 0.330418 0.293890 0.202701 0.555509 0.447660 -0.361554 -0.266283 -0.134947 0.105315 0.131263 0.548085 -0.195238 0.062958 -0.011117 -0.226676 0.050336 -0.295650 -0.201271 0.014450 0.026845 0.403077 -0.221277 -0.236224 0.213415 -0.163396 -0.218948 -0.242459 -0.346984 0.282615 0.014165 -0.342011 0.370489 -0.372362 0.102479 0.547047 0.020831 -0.202521 -0.180814 0.035923 -0.296322 -0.062603 0.232734 0.191323 0.251916 0.150993 -0.024009 0.129037 -0.033097 0.029713 0.125488 -0.018356 -0.226277 0.437586 0.004913]

这些数字本身并没有太多意义。但通过找出语料库中哪些其他词的向量最相似，我们可以运行一个函数，生成与"book"一词最相似的词。在这个网络中，结果是：book, books, essay, memoir, essays, novella, anthology, blurb, autobiography, audiobook。对计算机来说，这已经相当不错了！通过一堆本身不带任何特定含义的数字，AI 能够生成在含义上（而非字母或声音上）与"book"非常相似的词！我们也可以根据向量的差异计算词之间的差异。例如，king 和 man 之间的差异类似于 queen 和 woman 之间的差异。也就是说，如果我们将 king 和 man 之间的差异添加到 woman 的向量上，结果向量最接近的词是 queen！同样，如果我们将 ramen 和 japan 之间的差异添加到 america 上，我们得到 burritos。通过使用神经网络和词的分布式表示，我们使 AI 理解了语言中词之间的语义相似性，让 AI 更接近能理解和产生人类语言的目标。

## 神经网络

回顾一下，神经网络接收某些输入，将其传递给网络，并创建一些输出。通过向网络提供训练数据，它可以越来越准确地将输入转换为输出。通常，机器翻译使用神经网络。在实践中，当我们翻译词时，我们希望翻译一个句子或段落。由于句子长度固定，我们面临着将一个序列翻译成另一个大小不固定的序列的问题。如果你曾与 AI 聊天机器人交谈，它需要理解一系列词并生成适当的序列作为输出。

循环神经网络可以多次重新运行神经网络，跟踪保存所有相关信息的状态。输入被送入网络，创建隐藏状态。将第二个输入与第一个隐藏状态一起传递给编码器，产生新的隐藏状态。这个过程重复进行，直到传递结束标记。然后，解码状态开始，创建一个接一个的隐藏状态，直到我们得到最后一个词和另一个结束标记。然而，一些问题出现了。编码阶段的一个问题是来自输入阶段的所有信息必须存储在一个最终状态中。对于大型序列，将所有信息存储到单个状态值中非常具有挑战性。如果能以某种方式结合所有隐藏状态，将会很有用。另一个问题是输入序列中的一些隐藏状态比其他状态更重要。有没有办法知道哪些状态（或词）比其他状态更重要？

## 注意力机制

注意力是指神经网络决定哪些值比其他值更重要的能力。在句子"What is the capital of Massachusetts"中，注意力机制使神经网络能够决定在生成输出句子的每个阶段它将注意哪些值。进行这样的计算时，神经网络将显示，在生成答案的最后一个词时，"capital"和"Massachusetts"是最重要的注意对象。通过计算注意力得分，将其乘以网络生成的隐藏状态值，并将它们相加，神经网络将创建一个解码器可用于计算最终词的最终上下文向量。这类计算中出现的一个挑战是循环神经网络需要对词进行顺序训练。这需要大量时间。随着大型语言模型的增长，它们的训练时间越来越长。随着需要训练的数据集越来越大，对并行性的需求稳步增长。因此，引入了一种新的架构。

## Transformers

Transformers 是一种新型的训练架构，其中每个输入词都同时通过神经网络处理。输入词进入神经网络并被捕获为编码表示。由于所有词同时输入到神经网络中，词序很容易丢失。因此，位置编码被添加到输入中。因此，神经网络将在编码表示中同时使用词和词的位置。此外，添加了自注意力步骤来帮助定义输入词的上下文。实际上，神经网络通常会使用多个自注意力步骤，以便更好地理解上下文。这个过程会对序列中的每个词重复多次。结果是产生了编码表示，在解码信息时将非常有用。

在解码步骤中，前一个输出词及其位置编码被提供给多个自注意力步骤和神经网络。此外，多个注意力步骤接收来自编码过程的编码表示并提供给神经网络。因此，词能够相互关注。此外，并行处理成为可能，计算又快又准确。

## 总结

我们已经在各种情境下研究了人工智能。我们研究了 AI 如何寻找解决方案的搜索问题。我们研究了 AI 如何表示知识和创造知识。我们研究了 AI 在不确定情况下的表现。我们研究了优化，最大化和最小化函数。我们研究了机器学习，通过查看训练数据找出模式。我们学习了神经网络以及它们如何使用权重从输入到输出。今天，我们研究了语言本身以及如何让计算机理解我们的语言。我们只是触及了这个过程的表面。我们真诚地希望你喜欢与我们一起的这段旅程。这是 Python 人工智能导论。
